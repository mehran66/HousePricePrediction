{
    "contents" : "---\ntitle: 'Lab 3: Mammal Sleep Patterns'\nauthor: \"Mehran Ghandehari\"\ndate: \"February 15th, 2016\"\noutput: html_document\n---\n\n## Introduction\n\nA common objective of researchers is to determine whether two variables are associated with one another. Does patronage of a public facility vary with income? Does commuting vary with distance? Do housing prices vary with accessibility to major highways? Researchers are often interested in how variables *co-vary*. The correlation coefficient provies a standardized measure of just this: the linear association between two variables. However, it is important to remember that the existence of a linear association between two variables does not necessarily imply that there is a *causal* relationship between the two variables. If we are interested in causal relationships, then we must turn to regression analysis. Here, the relationship between one or more *independent* variables and a *dependent* variable can be explored.\n\nFor example, perhaps we are interested in the relationship between housing price and the size of a house (as measured by the number of finished square feet):\n\n```{r fig.width=5}\nsales = read.csv(\"Data/Milwaukee_Sales_2012.csv\")  # Load some data\ncor.test(sales$SalePrice, sales$FinSqft)  # Correlation test\nm = lm(SalePrice ~ FinSqft, data=sales)  # Fit a model\nplot(SalePrice ~ FinSqft, data=sales)  # Scatterplot...\nabline(m, col=\"red\")  # ...with best fit line\n```\n\nIn the following two-part lab exercise, we will utlize correlation (Part I) and regression analysis (Part II) techniques to explore the relationship between various measures used to describe the sleep patterns of different mammals. We will explore how these variables co-vary, as well as explore possible causal links between the sleep patterns of mammals and their physiological attributes.\n\nThe `SleepMammals` dataset describes a number of variables that describe the sleep patterns of different mammals:\n\nVariable    | Description\n------------|---------------\nBody Wt     | Body weight (kg)\nBrain Wt    | Brain weight (g)\nNonDreaming | Slow wave sleep (hrs/day)\nDreaming    | Paradoxial sleep (hrs/day)\nTotalSleep  | Total sleep (hrs/day)\nLifeSpan    | Maximum life span (yrs)\nGestation   | Gestation time (days)\nPredation   | Predation index (1=low, 5=high)\nExposure    | Exposure index (1=low, 5=high)\nDanger      | Danger index (1=low, 5=high)\n\n# Part I\n\n## Load in the Data\n\nThis data is in a format common to ecological surveys. It is not a regular csv file, and so care is required when loading into `R`. Normally, it is a good idea to inspect a dataset in a text editor (e.g., notepad) before loading data into into `R` so that you can figure out how the dataset is structured/formatted. Once we have an idea of the structure of the dataset, we can use the `read.table` command in `R` to get this loaded into a `data.frame`. Alternatively, we can use `Tools > Import Dataset > From Text File...` to help us figure out which parameters to use.\n\n```{r}\nmammals = read.table(\"Data/SleepMammals.txt\", header=TRUE)\nsummary(mammals)\n```\n\nNote that in the above summary, the values for `Predation`, `Exposure`, and `Danger` are treated as numeric (integers), whereas according to the above table, they should be treated as categorical data (`factors`). To fix this, we can convert those variables to `factors` using the `as.factor` function:\n\n```{r}\nmammals$Predation = as.factor(mammals$Predation)\nmammals$Exposure = as.factor(mammals$Exposure)\nmammals$Danger = as.factor(mammals$Danger)\n\n# Show a summary of these newly converted variables \nsummary(mammals[, c(\"Predation\", \"Exposure\", \"Danger\")])\n```\n\nAlternatively, we could have specified the column types in the `read.table` function using the `colClasses` argument (type `?read.table` into the console to find out more about the `read.table` function and its various arguments).\n\nNow that we have loaded the mammal sleep data into `R`, take a look at the variables and explore the dataset using the various summary tools available in `R`. Once you have a good idea of the structure of the dataset and the distribution of the variables, you should be able to answer the following questions to learn how to implement correlation, regression, and multiple regression techniques. We will also learn about making different types of plots/graphics.\n\n## Descriptive Statistics & Visualization\n\n1.\tCreate a new variable in the `mammals` `data.frame` named `BrainPct` that is the percentage of total body weight that the brain weighs. Note that the body weight is given in kilograms (kg) and brain weight is given in grams (g), so the calculation is `BrainPct = BrainWt / (BodyWt * 1000)`.\n    ```{r}\n    mammals$BrainPct = (mammals$BrainWt / (mammals$BodyWt * 1000))*100\n    ```\n\n  * Which animal has the highest `BrainPct`?\n    ```{r}\n    HighestBrainPct = mammals[mammals$BrainPct == max(mammals$BrainPct), \"Species\"]\n    print(HighestBrainPct)\n    \n    # or\n    # HighestBrainPct = mammals[which.max(mammals$BrainPct), \"Species\"]\n    ```\n\n  * Which animal has the lowest `BrainPct`?\n    ```{r}\n    LowestBrainPct = mammals[mammals$BrainPct == min(mammals$BrainPct), \"Species\"]\n    print(LowestBrainPct)\n    ```\n\n  * What is the mean and median of `BrainPct`?\n    ```{r}\n    # the mean of BrainPct\n    MeanBrainPct = mean(mammals$BrainPct)\n    print(MeanBrainPct)\n    \n    # the median of BrainPct\n    MedianBrainPct = median(mammals$BrainPct)\n    print(MedianBrainPct)\n    ```\n\n2. Take a look at the distribution of `BrainPct` by plotting a histogram.\n  * Try to produce an alternative plot (or two) that might also help you explore the overall distribution of this variable.\n    ```{r, fig.align='center'}\n    # distribution of BrainPct\n    par(mfrow=c(2,2))\n    hist(mammals$BrainPct, \n         col=\"gray\", \n         xlab = \"Brain Percentage\", \n         main = \"Histogram of Brain Percentage\")\n    \n    plot(density(mammals$BrainPct), \n         col=\"gray\", \n         main = \"Density plot of Brain Percentage\")\n    \n    boxplot(mammals$BrainPct, \n            col=\"gray\", \n            ylab = \"Brain Percentage\", \n            main = \"Box plot of Brain Percentage\")\n    \n    # vioplot is a combination of a density plot and a box plot\n    library(vioplot)\n    vioplot(mammals$BrainPct, col=\"gray\")\n    title(\"Vio plot of Brain Percentage\")\n    ```\n\n  * Does the distribution appear positively (right) skewed, negatively (left) skewed, or normally distributed?\n    ```{r}\n    # the distribution appears positively (right) skewed\n    ```\n\n  * Compute the [skewness statistic](https://en.wikipedia.org/wiki/Skewness) (you'll need the `moments` package to do this, if you don't have it... install it!). Does it indicate the distribution is skewed?\n    ```{r}\n    library(moments)\n    skewness(mammals$BrainPct)\n    # The skewness here is 1.315525. This value implies that the distribution of the data is skewed to the right or positively skewed. As a rule, positive skewness indicates that the mean is larger than the median, and the data distribution is right-skewed.\n    ```\n\n3. Create a scatterplot with `LifeSpan` on the y-axis and `Gestation` on the x-axis.\n    ```{r, fig.align='center'}\n    plot(mammals$Gestation, mammals$LifeSpan, \n       xlab = \"Gestation\",\n       ylab = \"LifeSpan\",\n       pch = 20,\n       col = \"grey\")\n    abline(lm(mammals$LifeSpan ~ mammals$Gestation), col=\"red\")\n    ```\n\n4.\tGenerate a boxplot that compares body weight across the five `Danger` levels. \n    ```{r, fig.align='center'}\n    boxplot(mammals$BodyWt ~ mammals$Danger,\n        col = \"beige\",\n        xlab = \"Danger Level\",\n        ylab = \"Body Weight\", outline =F) # The outliers have been eliminated\n\n    # ploting the mean of each group\n    means = aggregate(BodyWt~Danger,data=mammals,FUN=\"mean\", outline =F)\n    par(new=T)\n    points(means, col=\"red\",pch=18)\n    ```\n\n  * Finally, create a boxplot that compares `TotalSleep` across the five `Danger` levels.\n    ```{r, fig.align='center'}\n    boxplot(mammals$TotalSleep ~ mammals$Danger,\n        col = \"beige\",\n        xlab = \"Danger Level\",\n        ylab = \"Total Sleep\",\n        outline=FALSE) # The outliers have benn eliminated\n    \n    # ploting the mean of each group\n    means = aggregate(TotalSleep~Danger,data=mammals,FUN=\"mean\", outline =F)\n    par(new=T)\n    points(means, col=\"red\",pch=18)\n    ```\n\n  * What inferences can you make about the sleeping conditions of an animal from this box plot?\n    ```{r}\n    # From the the trend of mean and median across the five danger level, we can conclude that there is as indirect relationship between total sleep and denger level. That is, those mamals that are more under denger, should keep their eyes opens and sleep less. Also danger level 5 has the smallest range (smallest variance) of total sleep. \n    ```\n\n## Data Transformations\n\n5.\tLog-transform the `BrainPct` data and save it as a new variable `LogBrainPct`.\n    ```{r}\n    mammals$LogBrainPct = log(mammals$BrainPct)\n    ```\n\n  * Plot the histogram of `LogBrainPct` with a normal curve added (Hint: you'll want to use `freq=FALSE` in your `hist` function to plot densities instead of frequencies).\n    ```{r, fig.align='center' }\n    hist(mammals$LogBrainPct, \n         freq=FALSE, \n         col = \"beige\",\n         xlab = \"Log (Brain Percentage)\", \n         main = \"Logarithmic Histogram of Brain Percentage\")\n    \n    mu = mean(mammals$LogBrainPct)\n    std = sd(mammals$LogBrainPct)\n    curve(dnorm(x, mean=mu, sd=std), add=TRUE, col = \"red\")  # Add curve to plot\n    ```\n\n6. Discussion Question:\n  * What property of a data distribution are you attempting to change by taking the logarithm? Why do you think this is a useful thing to do? You'll need to refer to the literature and Google to answer this question.\n    ```{r}\n    # We use a data transformation (here, logarithm) for brain percentage that has a skewed distribution in order to make it normal. As we can see in the previous density plot, after logarithm transformation the brain percentage distribution is pretty much normal. This is useful for several reasons, such as changing an exponential correlation to a linear correlation, reducing skewness, convenience of data processing, meeting the assumptions of inferential statistics, making a relationship clear and the pattern in the data more interpretable.\n    ```\n    \n## Correlation Analysis\n\n7. What is the correlation between lifespan and gestation period? (Hint: there are some missing (`NA`) values here, you'll need to specify `use=\"complete.obs\"` when performing your correlation test (see `?cor` for details)) Could you have guessed this from your plot in question 3?\n    ```{r}\n    cor(mammals$LifeSpan, mammals$Gestation, use = \"complete.obs\", method = \"pearson\") # pearson method\n\ncor(mammals$LifeSpan, mammals$Gestation, use = \"complete.obs\", method =  \"kendall\") # kendall method\n\ncor(mammals$LifeSpan, mammals$Gestation, use = \"complete.obs\", method = \"spearman\") # spearman method\n\n    # Yes, from plot in question 3, it was clear that there is a positive linear relationship between these two variables.\n    ```\n    \n8. Compute the correlation matrix between total sleep, body weight, brain weight, life span and gestation. You'll need to subset your dataset to include only the above variables (Aside: you can get a `data.frame` with *only* `numeric` variables by using `df[sapply(x, is.numeric)]`).\n    ```{r, fig.align='center'}\n    MammalsSubset = data.frame(TotalSleep = mammals$TotalSleep, BodyWt = mammals$BodyWt, BrainWt = mammals$BrainWt, LifeSpan = mammals$LifeSpan, Gestation = mammals$Gestation)\n\n    # mammals[,sapply(mammals, is.numeric)] # this code is extract the numeric attributes, but here we don't need all of the numeric attributes\n\n    MammalsSubset_corr = cor(MammalsSubset, use = \"complete.obs\", method = \"pearson\") # get correlations\n    \n    library('corrplot') #package corrplot\n    corrplot(MammalsSubset_corr, method = \"circle\", addCoef.col = \"black\") #plot matrix\n\n# Use \"Pairs Plot\" from \"psych\" package to visualize scatter plot\n    library(\"psych\")\n    pairs.panels(MammalsSubset, gap = 0)\n    ```\n  * Based on Pearson's correlation coefficient, which two variables are *most* and *least* correlated with each-other?\n    ```{r}\n    # Body weight and brain weight are the most correlated (0.93) and body weight and life span are the least correlated (0.30).\n    ```\n\n# Part II\n\n## Linear Regression\n\n9. Create a linear regression model with `TotalSleep` as a dependent variable and `BodyWt` as the independent variable.\n    ```{r}\n    Sleep_BodyWt.lm = lm(mammals$TotalSleep ~ mammals$BodyWt)\n    coeffs = coefficients(Sleep_BodyWt.lm); coeffs\n    ```\n\n  * What is the regression equation ($y = \\beta_0 + \\beta_1 x_1$)?\n    ```{r}\n    # β0 = 10.835131637\n    # β1 = -0.001524093\n    # TotalSleep = 10.835131637 + (-0.001524093) BodyWt\n    ```\n\n  * Is `BodyWt` a significant predictor of `TotalSleep`? What evidence do you have to support this?\n    ```{r}\n    summary(Sleep_BodyWt.lm)\n    #  P-value is less than .05. so we can reject the null hypothesis; it means that there is a significant relationship between the variables in the linear regression model based upon the .05 significant level. But also we can accept the null hypothesis based on the .001 and .01 significant levels, and conclude that there is not a significant relationship between the body weight and total sleep. In addition, R-squared in the regression summary is a metric for evaluating the goodness of fit of your model. Values close to one mean that most of the variability in the dependent variable is explained by the regression model. On the other hand, values close to zero mean that the model cannot explains the variability of the response data around its mean. Here the value of R-squre is 0.09436, and so body weight cannot explains all of the variability of the total sleep in the the regression model. So, although body weight can explain some of the variability of the total sleep, it cannot explain most of the variability in the total sleep. Also the correlation between total sleep and body weight is -0.30, which is considered a low value and means there is not  strong relationship between these two variables. Furtherermore, based on the scatter plot of these two variables, we can understand that the small coefficient and correlation is due only to a few observation that can be considered as outliers.\n    ```\n\n  * Create a scatterplot for the model and add the regression line to the plot (Hint: you can add a regression line based on a fitted model using the `abline` function).\n    ```{r, fig.align='center'}\n    plot(mammals$BodyWt, mammals$TotalSleep,\n     main = \"linear regression model\",\n     xlab = \"BodyWt\",\n     ylab = \"TotalSleep\",\n     pch = 20,\n     col = \"grey\")\n    # Add fit lines\n    # Linear regression line (y ~ x) \n    abline(Sleep_BodyWt.lm, col=\"red\")\n    \n    par(mfrow=c(2,2))\n    plot (Sleep_BodyWt.lm) # linear regression model built-in plots\n    ```\n    \n10.\tCreate a linear regression model with `TotalSleep` as a dependent variable and `BrainWt` as the independent variable.\n    ```{r}\n    Sleep_BrainWt.lm = lm(mammals$TotalSleep ~ mammals$BrainWt)\n    coeffs = coefficients(Sleep_BrainWt.lm); coeffs\n    ```\n\n  * What is the regression equation ($y = \\beta_0 + \\beta_1 x_1$)?\n    ```{r}\n    # β0 = 11.016324028 \n    # β1 = -0.001719468 \n    # TotalSleep = 11.016324028  + (-0.001719468) BodyWt\n    ```\n\n  * Is `BrainWt` a significant predictor of `TotalSleep`? What evidence do you have to support this?\n    ```{r}\n    summary(Sleep_BrainWt.lm)\n    #  P-value (.00578) is less than .05. so we can reject the null hypothesis; it means that there is a significant relationship between the variables in the linear regression model based upon the .05 and .01 significant levels. But also we can accept the null hypothesis based on the .001 significant level, and conclude that there is not a significant relationship between the brain weight and total sleep. In addition, here the value of R-squre is 0.1282, and so brain weight cannot explains most of the variability of the total sleep in the the regression model. So although brain weight can explain some of the variability of the total sleep, cannot explains most of the variability of the total sleep. Also the correlation between total sleep and brain weight is -0.35, which is considered a low value and means that there is not such a strong relationship between these two variables. Furtherermore, based on the scatter plot of these two variables, we can understand that the small coefficient and correlation is due only to a few observation that can be considered as outliers.\n    ```\n\n  * Create a scatterplot for the model and add the regression line to the plot.\n    ```{r, fig.align='center'}\n    plot(mammals$BrainWt, mammals$TotalSleep,\n     main = \"linear regression model\",\n     xlab = \"BrainWt\",\n     ylab = \"TotalSleep\",\n     pch = 20,\n     col = \"grey\")\n    # Add fit lines\n    # Linear regression line (y ~ x) \n    abline(Sleep_BrainWt.lm, col=\"red\")\n    \n    par(mfrow=c(2,2))\n    plot (Sleep_BrainWt.lm) # linear regression model built-in plots\n    ```\n\n11. Which of `BodyWt` and `BrainWt` was a better predictor of `TotalSleep`?\n    ```{r}\n    # Based on the p-value, R-square, coefficient and correlation, BrainWt is a better predictor of TotalSleep\n    ```\n\n## Multiple Regression\n\n12. Fit a multiple regression model using total sleep as the dependent variable and body weight, brain weight, life span, and gestation period as the four independent variables.\n    ```{r}\n    MultiReg.lm = lm(mammals$TotalSleep ~ mammals$BodyWt + mammals$BrainWt + mammals$LifeSpan + mammals$Gestation)\n    ```\n\n  * What is the regression equation ($y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\beta_4 x_4$)?\n    ```{r}\n    # β0 = 14.0736660\n    # β1 = -0.0005791\n    # β2 = 0.0020740\n    # β3 = -0.0072159 \n    # β4 = -0.0289144\n    # TotalSleep = 14.0736660 + (-0.0005791)BodyWt + (0.0020740)BrainWt + (-0.0072159 )LifeSpan + (-0.0289144)Gestation\n    ```\n    \n  * Which independent variables were deemed to be significant predictors in the model above?\n    ```{r}\n    summary(MultiReg.lm)\n    # Gestation is deemed to a significant predictor with a very small p-value\n    ```\n    \n  * How have the regression coefficients changed from the models in the previous section to the multiple regression model above?\n    ```{r}\n    # regression coefficient for the BodyWt is nearly similar for both models, but regression coefficient for the BrainWt changed from a negative value (-0.0017) in linear regression to a positive value (0.0020) in multiple regression. Also the intercept increased a ittle bit. Therefore, the regression coefficients in a mltiple regresions are not equal to the correponding coefficients in the linear regresions. Also the p-values of t-statistics for both of the coefficients have increased alot. This means that these variables do have quite a weak effect in the regression model and moost of the variation in the total sleep has been modeled by gestation.\n    ```\n    \n13. Discussion Question: \n  * Why is such a linear modelling framework, where we combine multiple independent (explanatory) variables such a valuable tool for analysing datasets encountered in geography?\n    ```{r}\n    # Geographic phenomena are dependent on various variables, processes, and relationships. for example, flooded areas are a function of elevation, terrain slope, soil type, etc.; or car accidents are a function of weather, road conditions, speed, etc.; These relationships and processes can be discovered, modeled and explored using regression models (linear or non-linear). These models can be used to find the factors behind spatial patterns. They also can be used for prediction; By modeling a phenomena, a prediction model can be built for some other places and times that direct measurments are impossible. To understand the processes accuring on the earth, we need to find the realationship between valiables that influence these processes. So, regression moodels help to better understand the geographic phenomena. It can be used to model cause-effect processes as well. for example, understanding the the factors that increase the risk of an environmenral disaster. Finding relationships between variables and the degree of relationship is quite important in all of the geographuc studies.\n    ```\n\n### Reference\n\nThis lab has been adapted from a lab developed by [Dr. Jed Long](http://jedalong.github.io) at the University of St. Andrews. The introduction comes from Rogerson's [Statistical Methods for Geography](https://study.sagepub.com/rogerson4e), [Chap. 7](https://study.sagepub.com/rogerson4e/student-resources/correlation).\n\n### Acknowledgment\n\nThis lab was really helpful. Thanks!\n",
    "created" : 1455397235260.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "479830890",
    "id" : "223CEB3B",
    "lastKnownWriteTime" : 1455609326,
    "path" : "~/Desktop/Quantitative/Lab3/Lab3_Ghandehari_final.Rmd",
    "project_path" : "Lab3_Ghandehari_final.Rmd",
    "properties" : {
    },
    "relative_order" : 2,
    "source_on_save" : false,
    "type" : "r_markdown"
}