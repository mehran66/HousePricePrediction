library(car)
outlierTest(mod1) # Bonferonni p-value for most extreme obs
```
```{r eval=F}
" There are 1 outliers based on Bonferroni test (82). But 520, 235, 420, 1nd 433 also is apparantly outliers based on the regression plots.
"
```
4-3-1-3- pattern (Nonlinearity)
```{r}
# When the residuals bounce randomly around the 0 line, it suggests that the assumption that the relationship is linear is reasonable
# Evaluate Nonlinearity
# component + residual plot
crPlots(mod1)
```
```{r eval=F}
" Vandalism are not linear.
"
```
4-3-1-4- Normality of Residuals
```{r}
qqPlot(mod1, main="QQ Plot") #qq plot for studentized resid
# distribution of studentized residuals
library(MASS)
sresid <- studres(mod1)
hist(sresid, freq=FALSE,
main="Distribution of Studentized Residuals")
xfit<-seq(min(sresid),max(sresid),length=40)
yfit<-dnorm(xfit)
lines(xfit, yfit)
#Null hypothesis residuals are normally distributed
shapiro.test(resid(mod1))
```
```{r eval=F}
" Residuals are nearly normal. It seems there are some outliers.
"
```
4-3-1-5- Non-independence of Errors
```{r}
# Test for Autocorrelated Errors (verifies if the residuals from a linear model are correlated        or not)
library(car)
durbinWatsonTest(mod1)
# The null hypothesis (H0H0) is that there is no correlation among residuals, i.e., they are           independent.
# The alternative hypothesis (HaHa) is that residuals are autocorrelated.
```
```{r eval=F}
" Residuals are autocorrelated
"
```
4-3-2- map residuals
```{r echo=FALSE}
# When the residuals from a model show clear spatial patterns there is evidence of some sort of     missing variable.
mapClassifyFeatures(chicago, resid(mod1), 5, "jenks", "YlOrRd" )
```
```{r eval=F}
"  There is not a clear spatial pattern in the residual map.
"
```
4-3-3- check for multicollinearity (VIF) (Note: the cutoff is 2.5)
```{r }
# Evaluate Collinearity
library(car)
vif(mod1) # variance inflation factors
AIC(mod1) # 2042.116
mod1 = lm(adj_Burglary ~
(Percent_Less_than_High_School_Education_) +
sqrt(Weapons_Violation) +
sqrt(Vandalism),
data=chicago@data)
```
```{r}
summary(mod1)
mod1 = lm(adj_Burglary ~
(Percent_Less_than_High_School_Education_) +
(Weapons_Violation) +
sqrt(Vandalism),
data=chicago@data)
```
```{r}
summary(mod1)
library(lmtest)
bptest(mod1)
4-1- LM summary
```{r echo=FALSE}
mod1 = lm(adj_Burglary ~
(Percent_Less_than_High_School_Education_) +
(Weapons_Violation) +
sqrt(Vandalism),
data=chicago@data)
```
```{r}
summary(mod1)
VIF(mod1)
vif(mod1)
AIC(mod1)
mod1_final = lm(adj_Burglary ~
Percent_Less_than_High_School_Education_ +
Weapons_Violation +
sqrt(Vandalism),
data=chicago@data)
summary(mod1_final)
plot(sqrt(Weapons_Violation) ~ (Number_of_violent_crimes), data = chicago@data)
violent_crimes <- lm(sqrt(Weapons_Violation) ~ (Number_of_violent_crimes) , data = chicago@data)
summary(violent_crimes)
plot(sqrt(Weapons_Violation) ~ (Number_of_property_crimes), data = chicago@data)
violent_crimes <- lm(sqrt(Weapons_Violation) ~ Number_of_property_crimes, data = chicago@data)
summary(violent_crimes)
plot(sqrt(Weapons_Violation) ~ (Number_of_property_crimes), data = chicago@data)
plot(sqrt(Weapons_Violation) ~ sqrt(Number_of_property_crimes), data = chicago@data)
violent_crimes <- lm(sqrt(Weapons_Violation) ~ sqrt(Number_of_property_crimes), data = chicago@data)
summary(violent_crimes)
plot(sqrt(Weapons_Violation) ~ (Homicides), data = chicago@data)
violent_crimes <- lm(sqrt(Weapons_Violation) ~ (Homicides), data = chicago@data)
summary(violent_crimes)
plot(sqrt(Weapons_Violation) ~ sqrt(Homicides), data = chicago@data)
plot(sqrt(Weapons_Violation) ~ (Percent_of_children_in_single_female_headed_household), data = chicago@data)
violent_crimes <- lm(sqrt(Weapons_Violation) ~ (Percent_of_children_in_single_female_headed_household), data = chicago@data)
summary(violent_crimes)
plot(sqrt(Weapons_Violation) ~ (Percent_Hispanic_), data = chicago@data)
violent_crimes <- lm(sqrt(Weapons_Violation) ~ (Percent_Hispanic_), data = chicago@data)
summary(violent_crimes)
plot(sqrt(Weapons_Violation) ~ sqrt(Percent_Hispanic_), data = chicago@data)
violent_crimes <- lm(sqrt(Weapons_Violation) ~ sqrt(Percent_Hispanic_), data = chicago@data)
summary(violent_crimes)
plot(sqrt(Weapons_Violation) ~ (PCT_of_Households_with_cash_public_assistance), data = chicago@data)
violent_crimes <- lm(sqrt(Weapons_Violation) ~ (PCT_of_Households_with_cash_public_assistance), data = chicago@data)
summary(violent_crimes)
plot(sqrt(Weapons_Violation) ~ sqrt(Disorderly_Conduct), data = chicago@data)
violent_crimes <- lm(sqrt(Weapons_Violation) ~ (Disorderly_Conduct), data = chicago@data)
summary(violent_crimes)
summary(chicago$Weapons_Violation)
mapClassifyFeatures(chicago, chicago$Weapons_Violation, 7, "quantile", "YlGnBu" )
hist(chicago@data$Weapons_Violation, col = "beige", main = "number of violent crimes")
boxplot(chicago@data$Weapons_Violation,
col = "beige",
ylab = "number of violent crimes")
```
2-2- What general trends do you observe?
```{r eval=F}
"Based on the map of number of Weapons_Violation, we can see that this crime is not random and is clustered. Box plot shows that there are some outliers in the data."
```
### 3 - Data Preparation
3-1- Did you combine any fields, create dummy variables, transform variables, subset the data, etc?
```{r }
# Box plot shows that data is not normal and there are some outliers in the data. Square root transform is used to normalize the data
# Shapiro-Wilk normality test
shapiro.test(chicago@data$Weapons_Violation) # W = 0.92161, p-value < 2.2e-16 < 2.2e-16
shapiro.test(sqrt(chicago@data$Weapons_Violation)) # W = 0.99351, p-value = 0.005998 (better)
#Create a new variable called adj_Weapons_Violation that ‘adjusts’ for potential outliers.
shapiro.test(log(chicago@data$Weapons_Violation)) # W = 0.99351, p-value = 0.005998 (better)
chicago$adj_Weapons_Violation = sqrt(chicago$Weapons_Violation)
# if we map the transformed data, we can see a random spatial patterns.
```
### 4 - 1st Model Specification
mod1 = lm(adj_Weapons_Violation ~ Number_of_violent_crimes +
Number_of_property_crimes +
Homicides +
Percent_of_children_in_single_female_headed_household +
PCT_of_Households_with_cash_public_assistance +
Disorderly_Conduct,
data=chicago@data)
```
```{r}
summary(mod1)
"Most of the variables are significant, except, Number_of_violent_crimes, Percent_of_children_in_single_female_headed_household, and Disorderly_Conduct.
The R-square is fine (0.6204) that means our dependent variales explains msot of the variability of the adj_Weapons_Violation in the the regression model.
The p-value of our model is very small and it means that there is a significant relationship between the variables in the regression model.
Percent_of_children_in_single_female_headed_household and Percent_Less_than_High_School_Education_ coefficients seems sucpitious, because their increase lead to decrease in the violent crime
"
```
4-3- Model Diagnostics
4-3-1- Residual plots
```{r}
plot(mod1)
hist(resid(mod1))
plot(resid(mod1) ~ predict(mod1))
abline(0, 0) # the horizon
```
4-3-1-1- Heteroskedasticity (non-constant variance)
```{r}
# Evaluate heteroscedasticity
# If the test is positive (low p value), you should see if any transformation of the dependent variable helps you eliminate heteroscedasticity
library(zoo)
library(lmtest)
bptest(mod1)
There is not heteroscedasticity
" There is not heteroscedasticity
"
```
4-3-1-2- Influential observations (possible outliers)
```{r}
# Bonferroni p-values to assesse Outliers
library(car)
outlierTest(mod1) # Bonferonni p-value for most extreme obs
```
" There are 1 outliers based on Bonferroni test (601).
"
```
4-3-1-3- pattern (Nonlinearity)
```{r}
# When the residuals bounce randomly around the 0 line, it suggests that the assumption that the relationship is linear is reasonable
# Evaluate Nonlinearity
# component + residual plot
crPlots(mod1)
Number_of_violent_crimes  are not linear
not
not
" Number_of_violent_crimes  are not linear.
"
```
4-3-1-4- Normality of Residuals
```{r}
qqPlot(mod1, main="QQ Plot") #qq plot for studentized resid
# distribution of studentized residuals
library(MASS)
sresid <- studres(mod1)
hist(sresid, freq=FALSE,
main="Distribution of Studentized Residuals")
xfit<-seq(min(sresid),max(sresid),length=40)
yfit<-dnorm(xfit)
lines(xfit, yfit)
#Null hypothesis residuals are normally distributed
shapiro.test(resid(mod1))
```
" Residuals are nearly normal. It seems there are a few outliers.
"
```
4-3-1-5- Non-independence of Errors
```{r}
# Test for Autocorrelated Errors (verifies if the residuals from a linear model are correlated        or not)
library(car)
durbinWatsonTest(mod1)
Residuals are not autocorrelated
" Residuals are not autocorrelated
"
```
4-3-2- map residuals
```{r echo=FALSE}
# When the residuals from a model show clear spatial patterns there is evidence of some sort of     missing variable.
mapClassifyFeatures(chicago, resid(mod1), 5, "jenks", "YlOrRd" )
```
```{r eval=F}
"  There is not a clear spatial pattern in the residual map.
"
```
4-3-3- check for multicollinearity (VIF) (Note: the cutoff is 2.5)
```{r }
# Evaluate Collinearity
library(car)
vif(mod1) # variance inflation factors
The cuttoff is 2.5, and here one of them is higher than this cuttoff.
"
```
4-3-4- examine the partial R-Square for each variable
```{r }
library(lmSupport)
modelEffectSizes(mod1)
library(lmSupport)
modelEffectSizes(mod1)
mod1 = lm(adj_Weapons_Violation ~ Number_of_violent_crimes +
Number_of_property_crimes +
Homicides +
Percent_of_children_in_single_female_headed_household +
PCT_of_Households_with_cash_public_assistance +
Disorderly_Conduct,
data=chicago@data)
summary(seth.mdl)
mod1 = lm(adj_Weapons_Violation ~ Number_of_violent_crimes +
Number_of_property_crimes +
Homicides +
Percent_of_children_in_single_female_headed_household +
PCT_of_Households_with_cash_public_assistance +
Disorderly_Conduct,
data=chicago@data)
```
```{r}
summary(mod1)
mod1 = lm(adj_Weapons_Violation ~ Number_of_violent_crimes +
Homicides +
Percent_of_children_in_single_female_headed_household +
PCT_of_Households_with_cash_public_assistance +
Disorderly_Conduct,
data=chicago@data)
```
```{r}
summary(mod1)
mod1 = lm(adj_Weapons_Violation ~ Number_of_violent_crimes +
Percent_of_children_in_single_female_headed_household +
PCT_of_Households_with_cash_public_assistance +
Disorderly_Conduct,
data=chicago@data)
```
```{r}
summary(mod1)
mod1 = lm(adj_Weapons_Violation ~
Percent_of_children_in_single_female_headed_household +
PCT_of_Households_with_cash_public_assistance +
Disorderly_Conduct,
data=chicago@data)
```
```{r}
summary(mod1)
mod1 = lm(adj_Weapons_Violation ~ Number_of_violent_crimes +
Percent_of_children_in_single_female_headed_household +
PCT_of_Households_with_cash_public_assistance +
Disorderly_Conduct,
data=chicago@data)
```
```{r}
summary(mod1)
```
4-2- Interpret model coefficients/p-values, model rˆ2/F-Test
```{r eval=F}
"Most of the variables are significant, except, Number_of_violent_crimes, Percent_of_children_in_single_female_headed_household, and Disorderly_Conduct.
The R-square is fine (0.6204) that means our dependent variales explains msot of the variability of the adj_Burglary in the the regression model.
The p-value of our model is very small and it means that there is a significant relationship between the variables in the regression model.
Percent_of_children_in_single_female_headed_household and Percent_Less_than_High_School_Education_ coefficients seems sucpitious, because their increase lead to decrease in the violent crime
"
```
4-3- Model Diagnostics
4-3-1- Residual plots
```{r}
plot(mod1)
4-3-1-1- Heteroskedasticity (non-constant variance)
```{r}
# Evaluate heteroscedasticity
# If the test is positive (low p value), you should see if any transformation of the dependent variable helps you eliminate heteroscedasticity
library(zoo)
library(lmtest)
bptest(mod1)
```
```{r eval=F}
" There is heteroscedasticity
"
```
4-3-1-2- Influential observations (possible outliers)
```{r}
# Bonferroni p-values to assesse Outliers
library(car)
outlierTest(mod1) # Bonferonni p-value for most extreme obs
```
```{r eval=F}
" There are 1 outliers based on Bonferroni test (82). But 520, 235, 420, 1nd 433 also is apparantly outliers based on the regression plots.
"
```
4-3-1-3- pattern (Nonlinearity)
```{r}
# When the residuals bounce randomly around the 0 line, it suggests that the assumption that the relationship is linear is reasonable
# Evaluate Nonlinearity
# component + residual plot
crPlots(mod1)
mod1 = lm(adj_Weapons_Violation ~ sqrt(Number_of_violent_crimes) +
sqrt(Percent_of_children_in_single_female_headed_household) +
sqrt(PCT_of_Households_with_cash_public_assistance) +
sqrt(Disorderly_Conduct),
data=chicago@data)
```
```{r}
summary(mod1)
```{r}
summary(mod1)
```
4-2- Interpret model coefficients/p-values, model rˆ2/F-Test
```{r eval=F}
"Most of the variables are significant, except, Number_of_violent_crimes, Percent_of_children_in_single_female_headed_household, and Disorderly_Conduct.
The R-square is fine (0.6204) that means our dependent variales explains msot of the variability of the adj_Burglary in the the regression model.
The p-value of our model is very small and it means that there is a significant relationship between the variables in the regression model.
Percent_of_children_in_single_female_headed_household and Percent_Less_than_High_School_Education_ coefficients seems sucpitious, because their increase lead to decrease in the violent crime
"
```
4-3- Model Diagnostics
4-3-1- Residual plots
```{r}
plot(mod1)
4-3-1-1- Heteroskedasticity (non-constant variance)
```{r}
# Evaluate heteroscedasticity
# If the test is positive (low p value), you should see if any transformation of the dependent variable helps you eliminate heteroscedasticity
library(zoo)
library(lmtest)
bptest(mod1)
```
```{r eval=F}
" There is heteroscedasticity
"
```
4-3-1-2- Influential observations (possible outliers)
```{r}
# Bonferroni p-values to assesse Outliers
library(car)
outlierTest(mod1) # Bonferonni p-value for most extreme obs
```
```{r eval=F}
" There are 1 outliers based on Bonferroni test (82). But 520, 235, 420, 1nd 433 also is apparantly outliers based on the regression plots.
"
```
4-3-1-3- pattern (Nonlinearity)
```{r}
# When the residuals bounce randomly around the 0 line, it suggests that the assumption that the relationship is linear is reasonable
# Evaluate Nonlinearity
# component + residual plot
crPlots(mod1)
```
```{r eval=F}
" Vandalism are not linear.
"
```
4-3-1-4- Normality of Residuals
```{r}
qqPlot(mod1, main="QQ Plot") #qq plot for studentized resid
# distribution of studentized residuals
library(MASS)
sresid <- studres(mod1)
hist(sresid, freq=FALSE,
main="Distribution of Studentized Residuals")
xfit<-seq(min(sresid),max(sresid),length=40)
yfit<-dnorm(xfit)
lines(xfit, yfit)
#Null hypothesis residuals are normally distributed
shapiro.test(resid(mod1))
```
```{r eval=F}
" Residuals are nearly normal. It seems there are some outliers.
"
```
4-3-1-5- Non-independence of Errors
```{r}
# Test for Autocorrelated Errors (verifies if the residuals from a linear model are correlated        or not)
library(car)
durbinWatsonTest(mod1)
# The null hypothesis (H0H0) is that there is no correlation among residuals, i.e., they are           independent.
# The alternative hypothesis (HaHa) is that residuals are autocorrelated.
```
```{r eval=F}
" Residuals are autocorrelated
"
```
4-3-2- map residuals
```{r echo=FALSE}
# When the residuals from a model show clear spatial patterns there is evidence of some sort of     missing variable.
mapClassifyFeatures(chicago, resid(mod1), 5, "jenks", "YlOrRd" )
```
```{r eval=F}
"  There is not a clear spatial pattern in the residual map.
"
```
4-3-3- check for multicollinearity (VIF) (Note: the cutoff is 2.5)
```{r }
# Evaluate Collinearity
library(car)
vif(mod1) # variance inflation factors
```
```{r eval=F}
"
The cuttoff is 2.5, and here 2 of them are higher than this cuttoff and we should eliminate 1 of them.
Number_of_violent_crimes
Vandalism
"
```
4-3-4- examine the partial R-Square for each variable
```{r }
library(lmSupport)
modelEffectSizes(mod1)
```
mod1_red1 = lm(adj_Weapons_Violation ~ sqrt(Number_of_violent_crimes) +
sqrt(Percent_of_children_in_single_female_headed_household) +
sqrt(PCT_of_Households_with_cash_public_assistance) +
sqrt(Disorderly_Conduct),
data=chicago@data)
```
5-2- check for multicollinearity (VIF)
A VIF on reduced model is provided below. Removing these variables did improve the VIFs:
```{r echo=FALSE}
# Evaluate Collinearity
library(car)
vif(mod1_red1) # variance inflation factors
```
```{r eval=F}
"now Weapons_Violation and Vandalism are colinear.
"
```
5-3- Diagnostic tests: AIC, Anova, extra sum of squares (modelEffectSizes, partial R-Squared), etc.
```{r }
# Akaike Information Criterion (AIC)
# Small values are desirable
AIC(mod1) # 2042.116
AIC(mod1_red1) # 2040.859
mod1 = lm(adj_Weapons_Violation ~ Number_of_violent_crimes +
Number_of_property_crimes +
Homicides +
Percent_of_children_in_single_female_headed_household +
PCT_of_Households_with_cash_public_assistance +
Disorderly_Conduct,
data=chicago@data)
```
AIC(mod1) # 2042.116
AIC(mod1_red1) # 2040.859
anova(mod1, mod1_red1)
modelEffectSizes(mod1_red1)
mod1_red1 = lm(adj_Weapons_Violation ~ sqrt(Number_of_violent_crimes) +
sqrt(Percent_of_children_in_single_female_headed_household) +
sqrt(PCT_of_Households_with_cash_public_assistance) +
sqrt(Disorderly_Conduct),
data=chicago@data)
summary(mod1_red1)
mod1_red1 = lm(adj_Weapons_Violation ~ sqrt(Number_of_violent_crimes) +
sqrt(Percent_of_children_in_single_female_headed_household) +
sqrt(PCT_of_Households_with_cash_public_assistance) +
sqrt(Disorderly_Conduct),
data=chicago@data)
```
5-2- check for multicollinearity (VIF)
summary(mod1_red1)
